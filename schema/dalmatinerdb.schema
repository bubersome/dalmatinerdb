%% -*- erlang -*-

%% @doc The tcp port dalmatiner_db listens on for the tcp API
{mapping, "tcp_port", "dalmatiner_db.tcp_port",
 [{default, {{tcp_port}} },
  {datatype, integer}]}.

%% @doc Number of acceptor processes to keep around for tcp connections.
{mapping, "tcp_listeners", "dalmatiner_db.tcp_listeners",
 [{default, 100},
  {datatype, integer}]}.

%% @doc Number of allowed concurrent tcp connections.
{mapping, "tcp_max_connections", "dalmatiner_db.tcp_max_connections",
 [{default, 1024},
  {datatype, integer}]}.

%% @doc Maximum batch per vnode to be written at once on a tcp connection.
{mapping, "tcp.max_batch_size", "dalmatiner_db.max_bkt_batch_size",
 [{datatype, integer},
  {default, 500}]}.


%% @doc The number of Asyncronous worker for a vnode.
{mapping, "async_workers", "metric_vnode.async_workers",
 [{datatype, integer},
  {default, 5}]}.

%% @doc When handling handoffs how much should be the largest chunk be.
%% Large chunks improve thourhgput but too large chunks will cause
%% timeouts.
{mapping, "handoff_max_chunk_size", "metric_vnode.handoff_chunk",
 [{datatype, bytesize},
  {default, "1KB"}]}.

%% @doc The datapoints stored per file, default equals 1 week of data
%% in a 1 second resolution.
{mapping, "points_per_file", "metric_vnode.points_per_file",
 [{datatype, integer},
  {default, 604800}]}.

%% @doc How long data is kept by default, measured either in points
%% or infinity for not deleting old data. This can be changed on a
%% per bucket level using ddb-admin
{mapping, "lifetime", "metric_vnode.lifetime",
 [{datatype, [integer, {atom, infinity}]},
  {default, "infinity"}]}.

%% @doc Maximum number of files kept open per mstore, settings
%% over two have no effect, settings below 0 eqal to 0.
{mapping, "mstore.max_files", "metric_vnode.max_files",
 [{default, 2},
  {datatype, integer}]}.

%% @doc How often a pertition will be picked to be vacuumed.
{mapping, "vacuum_interval", "dalmatiner_vacuum.interval",
 [{default, "1h"},
  {datatype, {duration, ms}}]}.

%% @doc The path data gets stored into.
{mapping, "platform_data_dir", "riak_core.platform_data_dir",
 [{default, "{{platform_data_dir}}"},
  {datatype, string}]}.

%% @doc The path data gets stored into.
{mapping, "run_user_home", "setup.home",
 [{default, "{{run_user_home}}"},
  hidden,
  {datatype, string}]}.

%% @doc Default ring creation size.  Make sure it is a power of 2,
%% e.g. 16, 32, 64, 128, 256, 512 etc
%% This is set to a default of 4 to work in a one server installation
%% if you plan to expand your system please set it higher and reads
%% the manual at http://bit.ly/1ciKRkS
{mapping, "ring_size", "riak_core.ring_creation_size",
 [{datatype, integer},
  {default, 64},
  {commented, 64},
  {validators, ["ring_size"]}
 ]}.

{validator, "ring_size", "not a power of 2 greater than 1",
 fun(Size) ->
         Size > 1 andalso (Size band (Size-1) =:= 0)
 end}.

%% @doc The number of copies of the data that is keeped.
%% For good consistency N > (R + W) should be true.
{mapping, "n", "dalmatiner_db.n",
 [{datatype, integer},
  {default, 1}]}.

%% @doc The number of replies requried for a read request to succeed
%% The lower the value the faster the replies but the more likely stale
%% data is returend.
{mapping, "r", "dalmatiner_db.r",
 [{datatype, integer},
  {default, 1}]}.

%% @doc The number of replies requried for a write request to succeed
%% The lower the value the faster the write but the more likely to only
%% write partial data.
{mapping, "w", "dalmatiner_db.w",
 [{datatype, integer},
  {default, 1}]}.

%% @doc Schema directory
{mapping, "schema_dir", "riak_core.schema_dirs",
 [{default, "./share/schema"},
  {datatype, string}]
}.

{translation,
 "riak_core.schema_dirs",
 fun(Conf) ->
         D = cuttlefish:conf_get("schema_dir", Conf),
         [D]
 end
}.

%% @doc handoff.ip is the network address that Riak binds to for
%% intra-cluster data handoff.
{mapping, "handoff.ip", "riak_core.handoff_ip", 
 [{default, "127.0.0.1" },
  {datatype, string},
  {validators, ["valid_ipaddr"]}]
}.

%% @doc enable / disable self monitoring
{mapping, "self_monitor", "dalmatiner_db.self_monitor",
 [{default, on},
  {datatype, flag}]
}.
%% @doc Transport compression for sending data between the vnode
%% and the reading node.
{mapping, "metrics.transport_compression",
 "dalmatiner_db.metric_transport_compression",
 [{default, snappy},
  {datatype, {enum, [snappy, none]}}]}.

%% @doc Weather to exclude repairs for reads done over the most
%% recent period of time. This can significanty reduce the number
%% of reads repairs when queries are executed against very recent data.
{mapping, "read_repair.delay",
 "dalmatiner_db.read_repair_min_time",
 [{default, "1m"},
  {datatype, {duration, ms}}]}.

%% @doc Weather partial read repairs are done, this effects queries
%% that begin outside of the cutoff time but end within. If enabled
%% only the part of the query that is older then the cutoff date
%% are going to be repaird.
{mapping, "read_repair.partial_read_repair",
 "dalmatiner_db.partial_rr",
 [{default, on},
  {datatype, flag}]}.

%% @doc Number of asyncronous IO requests that won't block the vnode.
%% A highter number reduces blockage but can lead to piling up of requests.
{mapping, "io.max_async", "metric_vnode.max_async_io",
 [{datatype, integer},
  {default, 20}]}.

%% @doc How long a syncronous IO request is allowed to take before
%% considering it crashed.
{mapping, "io.timeout", "metric_vnode.sync_io_timeout",
 [{default, "30s"},
  {datatype, {duration, ms}}]}.


%% @doc Async reads are performed out of band of writes, this way
%% larger reads can be performed without blocking other actions.
{mapping, "io.parallel_reads", "metric_vnode.async_read",
 [{default, off},
  {datatype, flag}]}.

%% @doc The size of the async worker pool, more workers mean more possible
%% parallelism
{mapping, "io.parallel_reads.queue_size", "metric_vnode.io_queue_size",
 [{datatype, integer},
  {default, 5}]}.

%% @doc Minimal size of read requests to be considered for asyncronous
%% reads, this has no effect when async_reads is set to off
{mapping, "io.parallel_reads.min_size", "metric_vnode.async_min_size",
 [{datatype, integer},
  {default, 1000}]}.

%% @doc Some requests to the vnodes are handled by an asyncronous worker pool.
%% This parameter allows for tuning this pools behaviour when it comes dealing
%% with requests that are queued.
%% The default (fifo) will serve requests in the order they arrive at the worker
%% pool. The alternative is to serve the requests in the reverse order, dealing
%% with the most recent request first.
%% There are pro's and con's for both aproaches, it is best to test out what
%% works best for the desired characteristics.
%%
%% As a very rought rule of thumb:
%%  - fifo will lead to lower extremes
%%  - filo will lead to lower medians/mediums
{mapping, "io.parallel_reads.queue_strategy", "metric_vnode.queue_strategy",
 [{default, fifo},
  {datatype, {enum, [fifo, filo]}}]}.

%% @doc The cahe is bounded on size per vnode, metrics are cached
%% until the size limit is reached then the cache will start to evict
%% overflowing metrics, prioritizing old and large caches to reduce
%% the number of flushes needed.
{mapping, "vnode.cache.size", "metric_vnode.cache_size",
 [{datatype, bytesize},
  {default, "10MB"}]}.

%% @doc To organize data the cache uses a two layer hash table
%% while the lower layer is fixed tom 64 buckets the 1st layer can
%% be configured.
%% Choosing a larger number of buckets improves lookup times with
%% large number of metrics but trades it for longer times for
%% eviction and a larger (static) memory overhead.
{mapping, "vnode.cache.buckets", "metric_vnode.buckets",
 [{datatype, integer},
  {default, 128}]}.

%% @doc For each sub bucket a number of slots get reserved in the beginning
%% this reduces the allocation need at start up and allows the first metrics
%% to be introduced w/o needing to allocate the entry array.
%% The total entries allocated at startup can be calculated by:
%%
%% bucketys * 64 * initial_entries
{mapping, "vnode.cache.initial_entries", "metric_vnode.cache_initial_entries",
 [{datatype, integer},
  {default, 2}]}.


%% @doc For each data chunk in a metric a number of points is pre-allocated
%% this prevents the need to allocate data on every write. This datapoint array
%% is doubled in size every time it grows too big.
%%
%% This data is not preallocated when there is no metric or datapoint written!
{mapping, "vnode.cache.initial_data", "metric_vnode.cache_initial_data",
 [{datatype, integer},
  {default, 32}]}.



%% @doc The cach uses 3 generations to determine idle metrics
%% and prioritizes those for flushing during overflows.
%% Aging happens every n inserts, during the aging process gen2
%% gets merged into gen3 (more expensive), and gen1 gets copied to
%% gen2 (very cheap). Active metrics get pulled back into gen1.
%% The smaller the age cycle is the more often this happens.
{mapping, "vnode.cache.age_cycle", "metric_vnode.cache_age_cycle",
 [{datatype, integer},
  {default, 1000000}]}.

%% @doc The maximum gap allowed between two points that do not
%% cause a sparese segmetnt to be created.
%% Gaps means that the system will write 'undefined' points as
%% they are used to fill the gaps. This means that for extreme
%% out of order writes there is the chance that a defined datapoint
%% that was written before gets overwritten with an undefined one.
%% On the otherhand for spotty metrics that often drop a point or
%% two setting this too low will increase the number of write
%% operations as every gap will mean an additional one.
{mapping, "vnode.cache.max_gap", "metric_vnode.cache_max_gap",
 [{datatype, integer},
  {default, 10}]}.

